<!DOCTYPE HTML>
<html lang="en">

<head>
  <title>Junhwa Hur</title>

  <meta content="text/html; charset=utf-8" http-equiv="Content-Type">

  <meta name="author" content="Junhwa Hur" />
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="/style.css" />
  <link rel="canonical" href="http://localhost:4000/">
  <link href="https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet" type="text/css">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr style="padding:0px">
            <td style="padding:2.5%;width:70%;vertical-align:middle">
              <h1>
                Junhwa Hur
              </h1>
              <p>Hello! I received my Ph.D. at <a href="http://www.tu-darmstadt.de/index.en.jsp">Technische Universität Darmstadt</a>, where I was supervised by <a href="https://www.visinf.tu-darmstadt.de/visual_inference/people_vi/stefan_roth.en.jsp">Prof. Stefan Roth Ph.D.</a> at <a href="https://www.visinf.tu-darmstadt.de/visual_inference/index.en.jsp">Visual Inference group</a>. I received my M.Sc. at Seoul National University and B.Sc. at POSTECH.
              </p>
              <p style="text-align:center">
                <a href="mailto:junhwa.hur@gmail.com">Email</a> &nbsp;/&nbsp;
                <a href="/pdfs/cv.pdf">CV</a>  &nbsp;/&nbsp;
                <a href="https://scholar.google.com/citations?user=z4dNJdkAAAAJ">Google Scholar</a> &nbsp;/&nbsp;
                <a href="https://github.com/hurjunhwa">Github</a> &nbsp;/&nbsp;
                <a href="https://www.linkedin.com/in/junhwa-hur"> LinkedIn </a>
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
              <img style="width:100%;max-width:100%" alt="profile photo" src="images/profile_img.png">
            </td>
          </tr>
        </table>


<!-- News -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:2.5%;width:100%;vertical-align:middle">
              <h2>News</h2>
              <ul>
              <li>09/2022: Our paper on surround-view depth estimation has been accepted at NeurIPS 2022.</li>
              <li>06/2022: Our paper on robotic grasping for multiple general real-world objects has been accepted at IROS 2022.</li>
              <li>05/2022: Chosen as one of 157 outstanding reviewers of CVPR 2022.</li>
              <li>05/2022: Successfully defended my Ph.D. dissertation!</li></ul>
            </td>
          </tr>
        </table>

<!-- Research -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:2.5%;width:100%;vertical-align:middle">
              <h2>Research</h2>
              <p>
                I'm interested in computer vision and machine learning, especially understanding 3D dynamic scene from a monocular video through self-supervision.
              </p>
            </td>
          </tr>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          
          <tr onmouseout="surrounddepth_stop()" onmouseover="surrounddepth_start()">
            <td style="padding:2.0%;width:25%;vertical-align:middle">
              <div class="one" style="width:auto; height:auto; max-width:100%;">
                <div class="two" style="width:auto; height:auto; max-width:100%;" id='surrounddepth_mouseover'>
                <img src=/images/2022_vf.gif width="160"></div>
                <img src=/images/2022_vf.jpg width="160">
              </div>
              <script type="text/javascript">
                function surrounddepth_start() {
                  document.getElementById('surrounddepth_mouseover').style.opacity = "1";
                }
                function surrounddepth_stop() {
                  document.getElementById('surrounddepth_mouseover').style.opacity = "0";
                }
                surrounddepth_stop()
              </script>
            </td>
            <td style="padding:2.0%;width:75%;vertical-align:middle">              
              <h3>Self-Supervised Surround-View Depth Estimation with Volumetric Feature Fusion</h3>
              <br>
              Jung Hee Kim*, <b>Junhwa Hur</b>*, Tien Phuoc Nguyen, and Seong-Gyun Jeong
              <br>
              <em>*Equal contribution</font><br>
              <em>NeurIPS</em>, 2022
              <br><font color="gray">coming soon!</font>
              <p>We introduce a voxel-based approach to surround-view depth estimation that improves metric-scale depth accuracy and can synthesize a depth map at arbitrary rotated views.</p>

            </td>
          </tr>
          
          <tr onmouseout="phddissertation_stop()" onmouseover="phddissertation_start()">
            <td style="padding:2.0%;width:25%;vertical-align:middle">
              <div class="one" style="width:auto; height:auto; max-width:100%;">
                <div class="two" style="width:auto; height:auto; max-width:100%;" id='phddissertation_mouseover'>
                <video width="160" muted autoplay loop><source src=/images/comingsoon.png type="video/mp4">Your browser does not support the video tag. 
                </video></div>
                <img src=/images/comingsoon.png width="160">
              </div>
              <script type="text/javascript">
                function phddissertation_start() {
                  document.getElementById('phddissertation_mouseover').style.opacity = "1";
                }
                function phddissertation_stop() {
                  document.getElementById('phddissertation_mouseover').style.opacity = "0";
                }
                phddissertation_stop()
              </script>
            </td>
            <td style="padding:2.0%;width:75%;vertical-align:middle">              
              <a href="https://tuprints.ulb.tu-darmstadt.de/21624/"><h3>Joint Motion, Semantic Segmentation, Occlusion, and Depth Estimation</h3></a>
              <br>
              <b>Junhwa Hur</b>
              <br>
              
              <em>Ph.D. Dissertation, Technische Universität Darmstadt</em>, 2022
              <br>
              <a href="https://tuprints.ulb.tu-darmstadt.de/21624/">paper</a>
              <p>In this dissertation, we propose how to jointly formulate multiple tasks for scene understanding and what kind of benefits can be obtained from the joint estimation.</p>

            </td>
          </tr>
          
          <tr onmouseout="maskgrasp_stop()" onmouseover="maskgrasp_start()">
            <td style="padding:2.0%;width:25%;vertical-align:middle">
              <div class="one" style="width:auto; height:auto; max-width:100%;">
                <div class="two" style="width:auto; height:auto; max-width:100%;" id='maskgrasp_mouseover'>
                <video width="160" muted autoplay loop><source src=/images/comingsoon.png type="video/mp4">Your browser does not support the video tag. 
                </video></div>
                <img src=/images/comingsoon.png width="160">
              </div>
              <script type="text/javascript">
                function maskgrasp_start() {
                  document.getElementById('maskgrasp_mouseover').style.opacity = "1";
                }
                function maskgrasp_stop() {
                  document.getElementById('maskgrasp_mouseover').style.opacity = "0";
                }
                maskgrasp_stop()
              </script>
            </td>
            <td style="padding:2.0%;width:75%;vertical-align:middle">              
              <h3>MasKGrasp: Mask-based Grasping for Scenes with Multiple General Real-world Objects</h3>
              <br>
              Junho Lee, <b>Junhwa Hur</b>, Inwoo Hwang, and Young Min Kim
              <br>
              
              <em>IROS</em>, 2022
              <br><font color="gray">coming soon!</font>
              

            </td>
          </tr>
          
          <tr onmouseout="raftmsf_stop()" onmouseover="raftmsf_start()">
            <td style="padding:2.0%;width:25%;vertical-align:middle">
              <div class="one" style="width:auto; height:auto; max-width:100%;">
                <div class="two" style="width:auto; height:auto; max-width:100%;" id='raftmsf_mouseover'>
                <img src=/images/2022_raftmsf_b.jpg width="160"></div>
                <img src=/images/2022_raftmsf_a.jpg width="160">
              </div>
              <script type="text/javascript">
                function raftmsf_start() {
                  document.getElementById('raftmsf_mouseover').style.opacity = "1";
                }
                function raftmsf_stop() {
                  document.getElementById('raftmsf_mouseover').style.opacity = "0";
                }
                raftmsf_stop()
              </script>
            </td>
            <td style="padding:2.0%;width:75%;vertical-align:middle">              
              <a href="https://arxiv.org/abs/2205.01568"><h3>RAFT-MSF: Self-Supervised Monocular Scene Flow Using Recurrent Optimizer</h3></a>
              <br>
              Bayram Bayramli, <b>Junhwa Hur</b>, and Hongtao Lu
              <br>
              
              <em>arXiv preprint</em>, 2022
              <br>
              <a href="https://arxiv.org/abs/2205.01568">paper</a>
              <p>For self-supervised monocular scene flow, our RAFT-backbone-based approach significantly improves the scene flow accuracy and even outperforms a semi-supervised method.</p>

            </td>
          </tr>
          
          <tr onmouseout="multimonosf_stop()" onmouseover="multimonosf_start()">
            <td style="padding:2.0%;width:25%;vertical-align:middle">
              <div class="one" style="width:auto; height:auto; max-width:100%;">
                <div class="two" style="width:auto; height:auto; max-width:100%;" id='multimonosf_mouseover'>
                <video width="160" muted autoplay loop><source src=/images/2021_multimonosf.mp4 type="video/mp4">Your browser does not support the video tag. 
                </video></div>
                <img src=/images/2021_multimonosf.jpg width="160">
              </div>
              <script type="text/javascript">
                function multimonosf_start() {
                  document.getElementById('multimonosf_mouseover').style.opacity = "1";
                }
                function multimonosf_stop() {
                  document.getElementById('multimonosf_mouseover').style.opacity = "0";
                }
                multimonosf_stop()
              </script>
            </td>
            <td style="padding:2.0%;width:75%;vertical-align:middle">              
              <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Hur_Self-Supervised_Multi-Frame_Monocular_Scene_Flow_CVPR_2021_paper.pdf"><h3>Self-Supervised Multi-Frame Monocular Scene Flow</h3></a>
              <br>
              <b>Junhwa Hur</b> and Stefan Roth
              <br>
              
              <em>CVPR</em>, 2021
              <br>
              <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Hur_Self-Supervised_Multi-Frame_Monocular_Scene_Flow_CVPR_2021_paper.pdf">paper</a> / 
              <a href="https://openaccess.thecvf.com/content/CVPR2021/supplemental/Hur_Self-Supervised_Multi-Frame_Monocular_CVPR_2021_supplemental.pdf">supp</a> / 
              <a href="https://arxiv.org/abs/2105.02216">arxiv</a> / 
              <a href="https://github.com/visinf/multi-mono-sf">code</a> / 
              <a href="https://www.youtube.com/watch?v=nFLfm3YZ_RI">talk</a>
              <p>In the multi-frame setup, using ConvLSTM + warping hidden states improves the accuracy and the temporal consistency of the monocular scene flow.</p>

            </td>
          </tr>
          
          <tr onmouseout="monosf_stop()" onmouseover="monosf_start()">
            <td style="padding:2.0%;width:25%;vertical-align:middle">
              <div class="one" style="width:auto; height:auto; max-width:100%;">
                <div class="two" style="width:auto; height:auto; max-width:100%;" id='monosf_mouseover'>
                <video width="160" muted autoplay loop><source src=/images/2020_monosf.mp4 type="video/mp4">Your browser does not support the video tag. 
                </video></div>
                <img src=/images/2020_monosf.jpg width="160">
              </div>
              <script type="text/javascript">
                function monosf_start() {
                  document.getElementById('monosf_mouseover').style.opacity = "1";
                }
                function monosf_stop() {
                  document.getElementById('monosf_mouseover').style.opacity = "0";
                }
                monosf_stop()
              </script>
            </td>
            <td style="padding:2.0%;width:75%;vertical-align:middle">              
              <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Hur_Self-Supervised_Monocular_Scene_Flow_Estimation_CVPR_2020_paper.pdf"><h3>Self-Supervised Monocular Scene Flow Estimation</h3></a>
              <br>
              <b>Junhwa Hur</b> and Stefan Roth
              <br>
              
              <em>CVPR</em>, 2020<font color="#FA4841"><b>&nbsp;&nbsp;Oral presentation</b></font>
              <br>
              <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Hur_Self-Supervised_Monocular_Scene_Flow_Estimation_CVPR_2020_paper.pdf">paper</a> / 
              <a href="https://openaccess.thecvf.com/content_CVPR_2020/supplemental/Hur_Self-Supervised_Monocular_Scene_CVPR_2020_supplemental.pdf">supp</a> / 
              <a href="https://arxiv.org/abs/2004.04143">arxiv</a> / 
              <a href="https://github.com/visinf/self-mono-sf">code</a> / 
              <a href="https://www.youtube.com/watch?v=1lR6PQO82lc&feature=youtu.be">talk</a>
              <p>We propose to estimate scene flow from only two monocular images with a CNN trained in a self-supervised manner.</p>

            </td>
          </tr>
          
          <tr onmouseout="mhm_stop()" onmouseover="mhm_start()">
            <td style="padding:2.0%;width:25%;vertical-align:middle">
              <div class="one" style="width:auto; height:auto; max-width:100%;">
                <div class="two" style="width:auto; height:auto; max-width:100%;" id='mhm_mouseover'>
                <img src=/images/2020_mhm.jpg width="160"></div>
                <img src=/images/2020_mhm.jpg width="160">
              </div>
              <script type="text/javascript">
                function mhm_start() {
                  document.getElementById('mhm_mouseover').style.opacity = "1";
                }
                function mhm_stop() {
                  document.getElementById('mhm_mouseover').style.opacity = "0";
                }
                mhm_stop()
              </script>
            </td>
            <td style="padding:2.0%;width:75%;vertical-align:middle">              
              <a href="https://arxiv.org/abs/2004.02853"><h3>Optical Flow Estimation in the Deep Learning Age</h3></a>
              <br>
              <b>Junhwa Hur</b> and Stefan Roth
              <br>
              
              <em>Modelling Human Motion, N. Noceti, A. Sciutti and F. Rea, Eds., Springer</em>, 2020
              <br>
              <a href="https://link.springer.com/chapter/10.1007/978-3-030-46732-6_7">paper</a> / 
              <a href="https://arxiv.org/abs/2004.02853">arxiv</a>
              <p>As a book chapter, we comprehensively review CNN-based approaches to optical flow and their technical details, including un-/semi-supervised methods.</p>

            </td>
          </tr>
          
          <tr onmouseout="irr_stop()" onmouseover="irr_start()">
            <td style="padding:2.0%;width:25%;vertical-align:middle">
              <div class="one" style="width:auto; height:auto; max-width:100%;">
                <div class="two" style="width:auto; height:auto; max-width:100%;" id='irr_mouseover'>
                <video width="160" muted autoplay loop><source src=/images/2019_irr.mp4 type="video/mp4">Your browser does not support the video tag. 
                </video></div>
                <img src=/images/2019_irr.png width="160">
              </div>
              <script type="text/javascript">
                function irr_start() {
                  document.getElementById('irr_mouseover').style.opacity = "1";
                }
                function irr_stop() {
                  document.getElementById('irr_mouseover').style.opacity = "0";
                }
                irr_stop()
              </script>
            </td>
            <td style="padding:2.0%;width:75%;vertical-align:middle">              
              <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Hur_Iterative_Residual_Refinement_for_Joint_Optical_Flow_and_Occlusion_Estimation_CVPR_2019_paper.pdf"><h3>Iterative Residual Refinement for Joint Optical Flow and Occlusion Estimation</h3></a>
              <br>
              <b>Junhwa Hur</b> and Stefan Roth
              <br>
              
              <em>CVPR</em>, 2019
              <br>
              <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Hur_Iterative_Residual_Refinement_for_Joint_Optical_Flow_and_Occlusion_Estimation_CVPR_2019_paper.pdf">paper</a> / 
              <a href="http://openaccess.thecvf.com/content_CVPR_2019/supplemental/Hur_Iterative_Residual_Refinement_CVPR_2019_supplemental.pdf">supp</a> / 
              <a href="https://arxiv.org/abs/1904.05290">arxiv</a> / 
              <a href="https://github.com/visinf/irr">code</a>
              <p>An iterative residual refinement scheme based on weight sharing reduces the number of network parameters and improves the accuracy of optical flow and occlusion.</p>

            </td>
          </tr>
          
          <tr onmouseout="unflow_stop()" onmouseover="unflow_start()">
            <td style="padding:2.0%;width:25%;vertical-align:middle">
              <div class="one" style="width:auto; height:auto; max-width:100%;">
                <div class="two" style="width:auto; height:auto; max-width:100%;" id='unflow_mouseover'>
                <img src=/images/2018_unflow_flow.jpg width="160"></div>
                <img src=/images/2018_unflow_img.jpg width="160">
              </div>
              <script type="text/javascript">
                function unflow_start() {
                  document.getElementById('unflow_mouseover').style.opacity = "1";
                }
                function unflow_stop() {
                  document.getElementById('unflow_mouseover').style.opacity = "0";
                }
                unflow_stop()
              </script>
            </td>
            <td style="padding:2.0%;width:75%;vertical-align:middle">              
              <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/16502/16319"><h3>UnFlow: Unsupervised Learning of Optical Flow with a Bidirectional Census Loss</h3></a>
              <br>
              Simon Meister, <b>Junhwa Hur</b> and Stefan Roth
              <br>
              
              <em>AAAI</em>, 2018<font color="#FA4841"><b>&nbsp;&nbsp;Oral presentation</b></font>
              <br>
              <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/16502/16319">paper</a> / 
              <a href="https://arxiv.org/pdf/1711.07837.pdf">arxiv</a> / 
              <a href="https://github.com/simonmeister/UnFlow">code</a> / 
              <a href="http://simonmeister.org/files/slides_unflow_aaai18.pdf">slide</a>
              <p>By directly training on the target domain with an improved unsupervised loss, our method outperforms a supervised method that is pre-trained on a synthetic dataset.</p>

            </td>
          </tr>
          
          <tr onmouseout="mirrorflow_stop()" onmouseover="mirrorflow_start()">
            <td style="padding:2.0%;width:25%;vertical-align:middle">
              <div class="one" style="width:auto; height:auto; max-width:100%;">
                <div class="two" style="width:auto; height:auto; max-width:100%;" id='mirrorflow_mouseover'>
                <img src=/images/2017_mirrorflow_occ.png width="160"></div>
                <img src=/images/2017_mirrorflow.png width="160">
              </div>
              <script type="text/javascript">
                function mirrorflow_start() {
                  document.getElementById('mirrorflow_mouseover').style.opacity = "1";
                }
                function mirrorflow_stop() {
                  document.getElementById('mirrorflow_mouseover').style.opacity = "0";
                }
                mirrorflow_stop()
              </script>
            </td>
            <td style="padding:2.0%;width:75%;vertical-align:middle">              
              <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Hur_MirrorFlow_Exploiting_Symmetries_ICCV_2017_paper.pdf"><h3>MirrorFlow: Exploiting Symmetries in Joint Optical Flow and Occlusion Estimation</h3></a>
              <br>
              <b>Junhwa Hur</b> and Stefan Roth
              <br>
              
              <em>ICCV</em>, 2017
              <br>
              <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Hur_MirrorFlow_Exploiting_Symmetries_ICCV_2017_paper.pdf">paper</a> / 
              <a href="http://openaccess.thecvf.com/content_ICCV_2017/supplemental/Hur_MirrorFlow_Exploiting_Symmetries_ICCV_2017_supplemental.pdf">supp</a> / 
              <a href="https://arxiv.org/abs/1708.05355">arxiv</a> / 
              <a href="https://bitbucket.org/visinf/projects-2017-mirrorflow">code</a> / 
              <a href="/pdfs/2017_iccv_poster.pdf">poster</a>
              <p>The chicken-and-egg relationship between optical flow and occlusion can be nicely formulated through exploiting the symmetry properties they have.</p>

            </td>
          </tr>
          
          <tr onmouseout="jfs_stop()" onmouseover="jfs_start()">
            <td style="padding:2.0%;width:25%;vertical-align:middle">
              <div class="one" style="width:auto; height:auto; max-width:100%;">
                <div class="two" style="width:auto; height:auto; max-width:100%;" id='jfs_mouseover'>
                <img src=/images/2016_jfs_output.jpg width="160"></div>
                <img src=/images/2016_jfs_input.jpg width="160">
              </div>
              <script type="text/javascript">
                function jfs_start() {
                  document.getElementById('jfs_mouseover').style.opacity = "1";
                }
                function jfs_stop() {
                  document.getElementById('jfs_mouseover').style.opacity = "0";
                }
                jfs_stop()
              </script>
            </td>
            <td style="padding:2.0%;width:75%;vertical-align:middle">              
              <a href="https://arxiv.org/pdf/1607.07716v1.pdf"><h3>Joint Optical Flow and Temporally Consistent Semantic Segmentation</h3></a>
              <br>
              <b>Junhwa Hur</b> and Stefan Roth
              <br>
              
              <em>ECCV Workshop on Computer Vision for Road Scene Understanding and Autonomous Driving (ECCVW)</em>, 2016<font color="#FA4841"><b>&nbsp;&nbsp;Best paper award</b></font>
              <br>
              <a href="https://doi.org/10.1007/978-3-319-46604-0_12">paper</a> / 
              <a href="https://arxiv.org/pdf/1607.07716v1.pdf">arxiv</a> / 
              <a href="/pdfs/2016_eccvws_poster.pdf">poster</a>
              <p>We propose a method for the joint estimation of optical flow and temporally consistent semantic segmentation, which closely connects the two problem domains and allows each task leverage the other.</p>

            </td>
          </tr>
          
          <tr onmouseout="gdsp_stop()" onmouseover="gdsp_start()">
            <td style="padding:2.0%;width:25%;vertical-align:middle">
              <div class="one" style="width:auto; height:auto; max-width:100%;">
                <div class="two" style="width:auto; height:auto; max-width:100%;" id='gdsp_mouseover'>
                <video width="160" muted autoplay loop><source src=/images/2015_gdsp.mp4 type="video/mp4">Your browser does not support the video tag. 
                </video></div>
                <img src=/images/2015_gdsp.jpg width="160">
              </div>
              <script type="text/javascript">
                function gdsp_start() {
                  document.getElementById('gdsp_mouseover').style.opacity = "1";
                }
                function gdsp_stop() {
                  document.getElementById('gdsp_mouseover').style.opacity = "0";
                }
                gdsp_stop()
              </script>
            </td>
            <td style="padding:2.0%;width:75%;vertical-align:middle">              
              <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Hur_Generalized_Deformable_Spatial_2015_CVPR_paper.pdf"><h3>Generalized Deformable Spatial Pyramid: Geometry-Preserving Dense Correspondence Estimation</h3></a>
              <br>
              <b>Junhwa Hur</b>, Hwasup Lim, Changsoo Park, and Sang Chul Ahn
              <br>
              
              <em>CVPR</em>, 2015
              <br>
              <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Hur_Generalized_Deformable_Spatial_2015_CVPR_paper.pdf">paper</a> / 
              <a href="http://openaccess.thecvf.com/content_cvpr_2015/supplemental/Hur_Generalized_Deformable_Spatial_2015_CVPR_supplemental.pdf">supp</a> / 
              <a href="https://sites.google.com/site/hurjunhwa/research/gdsp">project</a> / 
              <a href="https://www.youtube.com/watch?v=74O5LiiXI6w">video</a>
              <p>A piece-wise similarity transform along pyramid levels can approximate a non-rigid deformation in the semantic matching problem.</p>

            </td>
          </tr>
          
          <tr onmouseout="3ddsp_stop()" onmouseover="3ddsp_start()">
            <td style="padding:2.0%;width:25%;vertical-align:middle">
              <div class="one" style="width:auto; height:auto; max-width:100%;">
                <div class="two" style="width:auto; height:auto; max-width:100%;" id='3ddsp_mouseover'>
                <img src=/images/2014_3ddsp.png width="160"></div>
                <img src=/images/2014_3ddsp.png width="160">
              </div>
              <script type="text/javascript">
                function 3ddsp_start() {
                  document.getElementById('3ddsp_mouseover').style.opacity = "1";
                }
                function 3ddsp_stop() {
                  document.getElementById('3ddsp_mouseover').style.opacity = "0";
                }
                3ddsp_stop()
              </script>
            </td>
            <td style="padding:2.0%;width:75%;vertical-align:middle">              
              <a href="https://doi.org/10.1007/978-3-319-14249-4_12"><h3>3D Deformable Spatial Pyramid for Dense 3D Motion Flow of Deformable Object</h3></a>
              <br>
              <b>Junhwa Hur</b>, Hwasup Lim, and Sang Chul Ahn
              <br>
              
              <em>International Symposium on Visual Computing (ISVC)</em>, 2014<font color="#FA4841"><b>&nbsp;&nbsp;Oral presentation</b></font>
              <br>
              <a href="https://doi.org/10.1007/978-3-319-14249-4_12">paper</a> / 
              <a href="https://sites.google.com/site/hurjunhwa/research/3d_dsp_14">project</a>
              

            </td>
          </tr>
          
          <tr onmouseout="iv2014_stop()" onmouseover="iv2014_start()">
            <td style="padding:2.0%;width:25%;vertical-align:middle">
              <div class="one" style="width:auto; height:auto; max-width:100%;">
                <div class="two" style="width:auto; height:auto; max-width:100%;" id='iv2014_mouseover'>
                <img src=/images/2014_iv.png width="160"></div>
                <img src=/images/2014_iv.png width="160">
              </div>
              <script type="text/javascript">
                function iv2014_start() {
                  document.getElementById('iv2014_mouseover').style.opacity = "1";
                }
                function iv2014_stop() {
                  document.getElementById('iv2014_mouseover').style.opacity = "0";
                }
                iv2014_stop()
              </script>
            </td>
            <td style="padding:2.0%;width:75%;vertical-align:middle">              
              <a href="http://dx.doi.org/10.1109/IVS.2014.6856537"><h3>Multi-lane Detection based on Accurate Geometric Lane Estimation in Highway Scenarios</h3></a>
              <br>
              Seung-Nam Kang, Soo-Mok Lee, <b>Junhwa Hur</b>, and Seung-Woo Seo
              <br>
              
              <em>Intelligent Vehicles Symposium (IV)</em>, 2014
              <br>
              <a href="http://dx.doi.org/10.1109/IVS.2014.6856537">paper</a> / 
              <a href="https://www.youtube.com/watch?v=RrxSSgeDdzk">video</a>
              

            </td>
          </tr>
          
          <tr onmouseout="iv2013_stop()" onmouseover="iv2013_start()">
            <td style="padding:2.0%;width:25%;vertical-align:middle">
              <div class="one" style="width:auto; height:auto; max-width:100%;">
                <div class="two" style="width:auto; height:auto; max-width:100%;" id='iv2013_mouseover'>
                <img src=/images/2013_iv.png width="160"></div>
                <img src=/images/2013_iv.png width="160">
              </div>
              <script type="text/javascript">
                function iv2013_start() {
                  document.getElementById('iv2013_mouseover').style.opacity = "1";
                }
                function iv2013_stop() {
                  document.getElementById('iv2013_mouseover').style.opacity = "0";
                }
                iv2013_stop()
              </script>
            </td>
            <td style="padding:2.0%;width:75%;vertical-align:middle">              
              <a href="http://dx.doi.org/10.1109/IVS.2013.6629645"><h3>Multi-lane Detection in Urban Driving Environments using Conditional Random Fields</h3></a>
              <br>
              <b>Junhwa Hur</b>, Seung-Nam Kang, and Seung-Woo Seo
              <br>
              
              <em>Intelligent Vehicles Symposium (IV)</em>, 2013
              <br>
              <a href="http://dx.doi.org/10.1109/IVS.2013.6629645">paper</a> / 
              <a href="https://sites.google.com/site/hurjunhwa/research/mld_iv13">project</a> / 
              <a href="https://github.com/hurjunhwa/mld_crf">code</a> / 
              <a href="https://www.youtube.com/watch?v=8LEUHUdqxag&feature=emb_title&ab_channel=JunhwaHur">video</a>
              

            </td>
          </tr>
          
          <tr onmouseout="msthesis_stop()" onmouseover="msthesis_start()">
            <td style="padding:2.0%;width:25%;vertical-align:middle">
              <div class="one" style="width:auto; height:auto; max-width:100%;">
                <div class="two" style="width:auto; height:auto; max-width:100%;" id='msthesis_mouseover'>
                <img src=/images/2013_ms_thesis.jpg width="160"></div>
                <img src=/images/2013_ms_thesis.jpg width="160">
              </div>
              <script type="text/javascript">
                function msthesis_start() {
                  document.getElementById('msthesis_mouseover').style.opacity = "1";
                }
                function msthesis_stop() {
                  document.getElementById('msthesis_mouseover').style.opacity = "0";
                }
                msthesis_stop()
              </script>
            </td>
            <td style="padding:2.0%;width:75%;vertical-align:middle">              
              <a href="/pdfs/2013_ms_thesis.pdf"><h3>Multi-lane Detection in Highway and Urban Driving Environment</h3></a>
              <br>
              <b>Junhwa Hur</b>
              <br>
              
              <em>Master’s thesis, Seoul National University</em>, 2013
              <br>
              <a href="/pdfs/2013_ms_thesis.pdf">paper</a> / 
              <a href="https://sites.google.com/site/hurjunhwa/research/mld_highway">project</a>
              

            </td>
          </tr>
          
        </table>
        <br>
        <br>

<!-- Project -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:2.5%;width:100%;vertical-align:middle">
              <h2>Project</h2>
            </td>
          </tr>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          
          <tr onmouseout="deformable_kist_stop()" onmouseover="deformable_kist_start()">
            <td style="padding:2.0%;width:25%;vertical-align:middle">
              <div class="one" style="width:auto; height:auto; max-width:100%;">
                <div class="two" style="width:auto; height:auto; max-width:100%;" id='deformable_kist_image'>
                  <img src=/images/2015_deformable.png width="160"></div>
                <img src=/images/2015_deformable.png width="160">
              </div>
              <script type="text/javascript">
                function deformable_kist_start() {
                  document.getElementById('deformable_kist_image').style.opacity = "1";
                }
                function deformable_kist_stop() {
                  document.getElementById('deformable_kist_image').style.opacity = "0";
                }
                deformable_kist_stop()
              </script>
            </td>
            <td style="padding:2.0%;width:75%;vertical-align:middle">
              <a href="https://www.youtube.com/watch?v=tJVxF_mcmZ4"><h3>Deformable Object Modeling</h3></a>
              <br>
              <em>IMRC, Korea Institute of Science and Technology (KIST)</em>, 2015 
              <br>
              <a href="https://www.youtube.com/watch?v=tJVxF_mcmZ4">video</a>
              <p></p>
              <p>Researching on modeling deformable object: pose estimation, correspondence search, and loop closure</p>

            </td>
          </tr>
          
          <tr onmouseout="korea_avc_stop()" onmouseover="korea_avc_start()">
            <td style="padding:2.0%;width:25%;vertical-align:middle">
              <div class="one" style="width:auto; height:auto; max-width:100%;">
                <div class="two" style="width:auto; height:auto; max-width:100%;" id='korea_avc_image'>
                  <img src=/images/2015_korea_avc.png width="160"></div>
                <img src=/images/2015_korea_avc.png width="160">
              </div>
              <script type="text/javascript">
                function korea_avc_start() {
                  document.getElementById('korea_avc_image').style.opacity = "1";
                }
                function korea_avc_stop() {
                  document.getElementById('korea_avc_image').style.opacity = "0";
                }
                korea_avc_stop()
              </script>
            </td>
            <td style="padding:2.0%;width:75%;vertical-align:middle">
              <a href="https://sites.google.com/site/hurjunhwa/research/korea_avc_13"><h3>Korea Autonomous Vehicle Contest 2013</h3></a>
              <br>
              <em>VILab, Seoul National University</em>, 2013 <font color="#FA4841"><b>&nbsp;&nbsp;2nd-place prize</b></font>
              <br>
              <a href="https://www.youtube.com/watch?v=8j-Ew4paNM0">video</a> / 
              <a href="https://sites.google.com/site/hurjunhwa/research/korea_avc_13">project</a>
              <p></p>
              <p>Researching on computer vision system for autonomous driving: lane/speed-bump/stop-line detection, camera-lidar sensor fusion</p>

            </td>
          </tr>
          
        </table>
        <br>
        <br>
        <br>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Design / source code from <a style="font-size:small;" href="https://jonbarron.info">Jon Barron</a>'s / <a style="font-size:small;" href="https://leonidk.com/">Leonid Keselman</a>'s website
              </p>
            </td>
          </tr>
        </table>
      </td>
    </tr>
  </table>
</body>

</html>

